# clustering
k-means, soft k-means, etc.

### Example 1. MNIST

Apply k-means to cluster the mnist dataset.

Dataset: https://www.kaggle.com/ngbolin/mnist-dataset-digit-recognizer

Because we actually have the true ground set, we can evaluate the quality of k-means using purity.

Result: <i>Purity [0..1] = 0.585</i>

### Example 2. SOFT-KMEANS

The raw implementation of soft-kmeans

Dataset is generated automatically by using blob with 5 clusters, 150 samples.

Soft-kmeans solves partially the sensitivity of initialization of k-means.

The following figure shows the result of clustering over iterations. The initial centroids are initialized randomly which are any data point in the dataset. These centroids would be updated over iterations until the convergence occurs.

<img src="https://github.com/ducanhnguyen/clustering/blob/master/img/blod_iterations.png" width="950">

### Example 3. WORD CLOUD

This example shows the following:

- how to choose the best number of clusters

- how to use word cloud

Dataset: https://archive.ics.uci.edu/ml/datasets/AAAI+2014+Accepted+Papers

The process is described as follows:

<b>Step 1.</b> Choose the best number of clusters (called K).

I only use column <i>groups</i> to find the best model of clusters. The information in the column <i>groups</i> shows the trend of research.

I ignore the column <i>abstract</i> because this column is too detail, which is not showed the clear trend. Similarly, column <i>title</i> should be ignored with the same reason.

<img src="https://github.com/ducanhnguyen/clustering/blob/master/img/wordcloud_num_cluster.png" width="650">

Based on the trend of inertia line and silhouette line, I choose K = 10 where occurs elbow.

<b>Step 2.</b> Plot the dataset on 2D with K = 10

Use PCA or tSNE to transform high dimensional dataset into 2d dataset. There are 10 clusters denoted from 0 to 9.

<img src="https://github.com/ducanhnguyen/clustering/blob/master/img/wordcloud_2d.png" width="650">

<b>Step 3.</b> Plot the word cloud of 10 clusters

I collected the extracted information in the column 'title', 'keywords', and 'abstract' to create word cloud.

<img src="https://github.com/ducanhnguyen/clustering/blob/master/img/wordcloud_.png" width="850">

<b>Step 4.</b> Search the phrases containing a word of a word cloud

For example, with the word <i>learning'</i>:

Result: ['machine LEARNING', 35], ['transfer LEARNING', 27], ['reinforcement LEARNING', 26], ['LEARNING ALGORITHMS', 24], ['metric LEARNING', 19], ['online LEARNING', 14], ['feature LEARNING', 14], ['active LEARNING', 14], ['semi-supervised LEARNING', 11], ['deep LEARNING', 11], ['dictionary LEARNING', 10], ['manifold LEARNING', 9], ['LEARNING ALGORITHM', 9], ['representation LEARNING', 8], ['multi-task LEARNING', 8], ['structure LEARNING', 7], ['multi-label LEARNING', 7], ['machine LEARNING algorithms', 7], ['LEARNING METHODS', 7], ['LEARNING APPROACH', 7], etc.

### Example 4. Gaussian mixture model

GMM tries to find the underlying distributions of observations. This is the raw implementation of <a href="http://cs229.stanford.edu/notes/cs229-notes7b.pdf">this tutorial</a>.

- Algorithm of gaussian mixture model

<img style='text-align: center' src="https://github.com/ducanhnguyen/clustering/blob/master/img/gmm_algo.png" width="450">

- The likelihood of GMM is as follows. We try to maximize this formula.

<img src="https://github.com/ducanhnguyen/clustering/blob/master/img/gmm_loss.png" width="450">

- Results: GMM converges to local minima after nearly 100 iterations with K = 3 (i.e., K is the number of Gaussian distributions). Let look at the two figures below to evaluate the performance of GMM. The left image is the original clusters, which each is generated by a gaussian distribution. The right image shows the three gaussian distributions which are found by GMM.

<table>
<tr>
<td><img src="https://github.com/ducanhnguyen/clustering/blob/master/img/gmm_before.png" width="650"></td>
<td><img src="https://github.com/ducanhnguyen/clustering/blob/master/img/gmm_after.png" width="650"></td>
  </tr>
</table>

- As you can see, the likelihood of GMM increases over iterations.

<img src="https://github.com/ducanhnguyen/clustering/blob/master/img/gmm_likelihood.png" width="450">




